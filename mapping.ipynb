{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple tutorial to map a model to the AKIDA1000 hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 18:50:28.576558: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-08 18:50:28.632174: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-08 18:50:28.632208: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-08 18:50:28.633869: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-08 18:50:28.642932: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-08 18:50:28.643483: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-08 18:50:29.831912: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from quantizeml.models import quantize\n",
    "from cnn2snn import convert\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cnn2snn\n",
    "import akida as ak\n",
    "from quantizeml.models import quantize\n",
    "from quantizeml.layers import QuantizationParams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the akida env version to v1 is very important to port it to hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "    TF version:  2.15.0\n",
      "MetaTF version:  2.11.0\n",
      " Akida version:  AkidaVersion.v1\n",
      "----------------------------------\n",
      "/home/kannan/anaconda3/envs/tf_env/bin/python\n",
      "Time of run:  2025-11-8_18-50\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CNN2SNN_TARGET_AKIDA_VERSION\"] = \"v1\"\n",
    "print('----------------------------------')\n",
    "print(\"    TF version: \", tf.__version__)\n",
    "print(\"MetaTF version: \", ak.__version__)\n",
    "print(' Akida version: ', cnn2snn.get_akida_version()) \n",
    "print('----------------------------------')\n",
    "\n",
    "!which python\n",
    "\n",
    "today = datetime.datetime.now()\n",
    "todayAsStr = str(today.year) + '-' + str(today.month) + '-' + str(today.day) + '_' + str(today.hour) + '-' + str(today.minute)\n",
    "print('Time of run: ', todayAsStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"akida_friendly_cnn_v5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 60, 76, 1)]       0         \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 60, 76, 16)        416       \n",
      "                                                                 \n",
      " conv1_relu (ReLU)           (None, 60, 76, 16)        0         \n",
      "                                                                 \n",
      " pool1 (MaxPooling2D)        (None, 30, 38, 16)        0         \n",
      "                                                                 \n",
      " conv2 (Conv2D)              (None, 30, 38, 32)        4640      \n",
      "                                                                 \n",
      " conv2_relu (ReLU)           (None, 30, 38, 32)        0         \n",
      "                                                                 \n",
      " pool2 (MaxPooling2D)        (None, 15, 19, 32)        0         \n",
      "                                                                 \n",
      " conv3 (Conv2D)              (None, 15, 19, 64)        18496     \n",
      "                                                                 \n",
      " conv3_relu (ReLU)           (None, 15, 19, 64)        0         \n",
      "                                                                 \n",
      " pool3 (MaxPooling2D)        (None, 8, 10, 64)         0         \n",
      "                                                                 \n",
      " conv_tail (Conv2D)          (None, 8, 10, 64)         4160      \n",
      "                                                                 \n",
      " conv_tail_relu (ReLU)       (None, 8, 10, 64)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 5120)              0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 16)                81936     \n",
      "                                                                 \n",
      " fc1_relu (ReLU)             (None, 16)                0         \n",
      "                                                                 \n",
      " logit (Dense)               (None, 1)                 17        \n",
      "                                                                 \n",
      " output (Activation)         (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109665 (428.38 KB)\n",
      "Trainable params: 109665 (428.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1. Load trained model \n",
    "model_src = keras.models.load_model(\"ESA_cnn.h5\", compile=False)\n",
    "model_src.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 340ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kannan/anaconda3/envs/tf_env/lib/python3.10/site-packages/cnn2snn/quantizeml/blocks.py:160: UserWarning: Conversion stops at layer logit because of a dequantizer. The end of the model is ignored:\n",
      "___________________________________________________\n",
      "Layer (type)\n",
      "===================================================\n",
      "output (Activation)\n",
      "===================================================\n",
      "\n",
      "  warnings.warn(\"Conversion stops\" + stop_layer_msg + \" because of a dequantizer. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compatibility issues:  None\n"
     ]
    }
   ],
   "source": [
    "compatibilityIssues = cnn2snn.check_model_compatibility(model_src, device=ak.AKD1000())\n",
    "print('Compatibility issues: ', compatibilityIssues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantizeml.models import quantize\n",
    "from quantizeml.layers import QuantizationParams\n",
    "\n",
    "# Quantize the model\n",
    "qparams = QuantizationParams(input_weight_bits=8, weight_bits=4, activation_bits=4, per_tensor_activations=True)\n",
    "model_quantized = quantize(model_src, qparams=qparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' Akida version check: ', cnn2snn.get_akida_version()) \n",
    "akida_model = cnn2snn.convert(model_quantized, file_path='akida_model.fbz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "akida_model = cnn2snn.convert(model_quantized, file_path='akida_model.fbz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model version: ', akida_model.ip_version, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
